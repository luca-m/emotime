\section{Project Architecture}

The project is composed of the following entities:

\begin{description}
  \item[FaceDetector:] a utility class that extracts faces from images.
  \item[GaborBank:] generator of features using Gabor filters.
  \item[Classifier:] generic 2-class classifier, which can be
    trained to predict a feature matrix.
  \item[EmoDetector:] using a set of classifiers, it realizes multi-class
    detection for emotions.
  \item[GUI:] generic GUI that uses an EmoDetector to predict the emotion.
\end{description}

Instead of creating a single executable that does everything, we chose a
\emph{linux-like} approach of having a set of tools, where each of them
performs a single task.

Thus, we created a set of efficient tools, written in \code{C++}, which performs
training and detection, while high-level tasks such as data and training
organization are done by \code{Python} scripts.

\subsection{Face detection}

The face detection task is realized by the class \code{FaceDetector}. It uses
trained \emph{Haar Cascades} classifiers to detect and crop the first face
from an image.

Since the face can be partially occluded or rotated around the principal axis,
mere detection is not enough. We implemented only the roll correction, by using
another \emph{Haar Cascades} classifier to detect the eyes coordinates. If
the eyes are not on the same axis, the image is rotated.

The face detection receives an image as an input and outputs a matrix containing
the cropped and adjusted face.

\subsection{Gabor bank}

The class \code{GaborBank} is a generator and container of \code{GaborKernel}.
It permits the generation of various Gabor kernels with different orientations,
dimensions and wavelengths.

Once filled, \code{GaborBank} can filter images, returning a matrix that
contains a stack of images, one for each filter present in the bank.

\subsection{Classifier}

\code{Classifier} is a generic 2-class classifier, which can be trained to
predict the label of a feature matrix. The specializations classes we implemented are
\code{SVMClassifier} and \code{AdaBoostClassifier}.

The training set is composed of \emph{negative} (class 0) and \emph{positive}
(class 1) samples. The output is a trained classifier. We used the
\emph{OpenCV}, \code{CvSVM} and \code{CvBoost}, which permit to save and load
their internal state.

\subsection{Emotion detector}

Once the 2-class classifiers are trained, they can be used together to realize
a multi-class classifiers. This goal is achieved by \code{EmoDetector}. It is
composed of multiple \code{Classifier} and combines their prediction in various
ways to detect the emotion.

The input is an image, which is preprocessed: the face is extracted and then
features are computed. Then result is the detected emotion with its prediction
score.

\subsection{GUI}

The GUI is realized using the \emph{OpenCV} API. A generic GUI (\code{AGui}) is
composed of a \code{FacePreProcessor}, which extracts features from images, an
\code{EmoDetector} and a \code{ACapture}.

An \code{ACapture} is a generic frame capture. The specializations we
implemented permit the capture from videos, images and webcams.

\subsection{Tools and scripts}

Tools are \code{C++} programs performing high-performance tasks. In order
to be generic, they require every algorithm parameter as input argument.

\begin{description}
  \item[facecrop\_cli] reads an image and outputs a new image with the cropped
    face. It also performs eye-adjusting.
  \item[gaborbank\_cli] reads an image and outputs a file containing the
    extracted features.
  \item[train\_cli] reads a training file, trains a classifier and writes the
    trained state in an output file in \emph{xml} format.
  \item[emo\_detector\_cli] reads images and detects emotions using trained
    classifiers.
  \item[emotimegui\_cli] captures the webcam and performs real-time emotion
    detection.
\end{description}

The duty of python scripts is to organize the training process, by using the
\code{C++} tools. The configuration, such as tools name and algorithm
parameters, are read from a configuration file.

\begin{description}
  \item[datasetInit.py] initializes the directory hierarchy needed by the
    training process. This directory is called \code{dataset} directory.
  \item[datasetFillCK.py] fills the \code{dataset} with images from the CK database.
  \item[datasetCropFaces.py] applies the facecrop tool to the images, saving
    them in the \code{dataset}.
  \item[datasetFeatures.py] uses the gaborbank tool to extract features from
    the faces. The features are saved as \emph{yml} files.
  \item[datasetPrepTrain.py] prepares the training files by combining the
    features files using various strategies. The outputs are \emph{csv} files
    containing the positive and negative samples.
  \item[datasetTrain.py] iterates the \emph{csv} training files and launches a
    multi-threading training using the train tool. Its outputs are trained
    classifiers, which are saved in the \code{dataset} as \code{xml} files.
  \item[datasetVerifyPrediction.py] uses the emo\_detector tool to verify the
    prediction of the trained classifiers. We don't have a \emph{validation
    set}, so we just test the prediction on the \emph{training set}.
\end{description}
