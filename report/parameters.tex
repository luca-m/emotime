\section{Parameters}

In this section, we will discuss the parameter used for the algorithm adopted in the class project.

\subsection{Face Detection Parameters}

OpenCV supports several parameters for multi-scale detection, from the classifier to use, to scale factor and minimum object size. We adopted an empirical approach and used quick-and-dirty python script to test face detection parameters directly using OpenCV implementation: we figured out that considering a 640$\times$490 figure as the minimum face size, size of 120$\times$200 produces good detection on available samples (face/image area ratio around 7-10\%).
But in order to ensure detection for all treatable faces, we set up a minimum size of 30$\times$60. This value is related to the resizing performed before feature extraction, in our experiments we chose 48$\times$48.

Regarding the eye detection step, we decided to set the minimum object threshold to a dynamical value calculated using knowledge of face area size and a qualitative estimation of the human face proportions. In detail, we considered that the width of an eye cannot be smaller than $\frac{1}{5}$ of the face width.

Also we tried several face detectors in order to see which one fits better for our purposes, for example we considered the \code{haarcascade\_frontalface\_ default.xml} and the \code{haarcascade\_frontalface\_cbcl1.xml}. The latter is interesting because it detects the inner part of the face, discarding the surroundings. This choice has side effects on results~\ref{res:issues}.

\subsection{Gabor Kernels Parameters}

Gabor parameter selection is one of the most delicate parts of the tuning process, it directly affects the features evaluated for the emotional state classification.
%Hopefully \cite{Littlewort04dynamicsof, Bartlett06fullyautomatic, Lades93distortioninvariant} provide some hints on the Gabor bank parameters that can be used to achieve good results, in detail the number of $\lambda$ (wavelength) to use, number of $\theta$ (orientation) and some clues about interesting wavelength ranges are provided:
In our experimentation, we decided to generate Gabor filter banks using relationship reported in previous section, and we have empirically fixed following parameters domain:

\begin{itemize}
  \item bandwidth $b \in 1.3,1.6$, %around $\sim \pi \frac{2}{5}, \frac{\pi}{2} $,
    ranging in common values of bandwidth\cite{gaborBandwidth}\footnote{\url{http://www.cs.rug.nl/~imaging/simplecell.html}}.
  \item $\sigma \in 1,6$, and consequentially $\lambda = \frac{\lambda}{\sigma} \sigma $
  \item $\theta \in ( 0 \ldots \frac{\pi}{2} )$
  \item kernel size determined using aspect ratio and scale of the Gaussian support ($\frac{\sigma}{\gamma}$), using the parameters above results from $5$ to $25$ pixels.
%\item $\lambda \in ( \frac{\pi}{32} \ldots \frac{\pi}{2} ) $ as suggested in \cite{Lades93distortioninvariant}
%\item suggested 5 $\lambda$ and 8 $\theta$ subdivisions
%\item same aspect ration $\gamma$ for each kernel in the bank
\end{itemize}

We also adopted a flexible approach by keeping subdivision number configurable in order to be able to refine parameters in further development stages.
%Least, no clear indication about the size of the kernel, so we decided to support multiple kernel size ranging from $7$ to $17$, this values have been fixed in the first testing phase, using the developed Gabor utilities for visualizing the resulting bank.

\subsection{Boosting Parameters}

Boosting parameters were considered in the early development stages and were ignored in the later phases due to the huge amount of time needed for \code{AdaBoost} training. However parameter choice involves:

\begin{itemize}
\item Variant of the algorithm to use, in our case \code{Real AdaBoost}, \code{Discrete AdaBoost} and \code{Gentle AdaBoost}.
\item Classifier trim weight, as the lower accepted weight for a weak classifier.
\item Depth of the trained decision tree.
\end{itemize}

We have no clue about these parameters so we tried several configurations and better results have been obtained via \emph{Gentle AdaBoost}, trimming weak classifiers provides a speed-up but it is a sensible operation due to distribution of classifier weights. However, as reported in~\ref{res:issues}, we have temporarily dismissed the boosting approach due to lack of time (and/or servers).

\subsection{SVM Parameters}

Linear SVM with soft max has only one parameter, the value \emph{C}, which
defines how much to consider the misclassification. Since we hadn't any
validation set, we couldn't verify how much this value could affect the train.
We decided to leave $C=0.5$, for no particular reason.

\subsection{Multi-class Strategies}

Each classification algorithm adopted works only for binary classification tasks, and our problem involves 7 distinct classes. For this reason, we follow the indications in \cite{Littlewort04dynamicsof, Bartlett06fullyautomatic} and implement a quite general support, building a voting scheme based multi-class classifier which relies on binary classifiers.
In detail, we support multi-class training and detection in the following configurations:

\begin{itemize}
\item \emph{1 vs 1}, binary classifiers separate single classes from each other.
\item \emph{1 vs all}, separates a class from the others.
\item \emph{many vs many}, separates groups of classes.
\end{itemize}

Indications in papers suggest that \emph{1 vs all} and \emph{many vs many} should be better approaches.